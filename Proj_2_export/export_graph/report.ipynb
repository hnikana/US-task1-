{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Data Preprocessing and Analysis of Missing , Zero and Inf values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2- Reading Row Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> CDJR_Financial_Data.txt , CDJR_Inventory_Data_2.txt , CDJR_SlsEff_data_2.txt were read and concatenated together. <br>\n",
    "A unique id has been created ; **dealercode_yr_mth<>** , this feature is called ***'id'***.<br>\n",
    "Testset; records associated to **2019**,  and Trainset(below 2019)have been splitted. <br>\n",
    "Via **pivoting** around id , any available acoount has been read for each id. The shape of whole row data is **(68640 , 267)** .<br> \n",
    "<br>\n",
    "<font color=blue> **Observation :**</font> <br> 1- Number of keys in excel file is 286 however 268 could be detected from reading the data, meaning that some accounts do not exist in row data and consequently some KPIs would be missing and would have nan values or inf values <br> 2- EXP23 and EXP3 could be found in trainset (yrs below 2019) but they are not in 2019 records and they were dropped and conseqiently some KPIs would be missing again , Those are ***Sales Effectiveness - CJDRCompactCar\tMV3 / EXP3 , Sales Effectiveness - CJDRStandardMidSizeCar\tMV23 / EXP23 , Inventory Per Expected - CJDRCompactCar\tINV3 / EXP3 , Inventory Per Expected - CJDRStandardMidSizeCar\tINV23 / EXP23***. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2- Analysis of missing and zero values at ACCOUNT level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The graph below shows the percentage of **Missing Values** <br><font color = red> Red bars </font> shows the accounts with missing values more than 30%.\n",
    "![Missing Values frequency](./data_missingvalue_frequency.png) <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As it could be seen, it seems there are some account which is missing almost for everyone. The figure below is showing the same story for zero values and it could be seen that lots of accounts are having a high frequency of zero values. <br>![Zero Values frequency](./data_zero_frequency.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> And finally , The figure below is showing the accumulative percentage of missing values and zero values. Those with more than <font color = red> **30%**</font> of having this combination, will be dropped. <br> ![Zero and Missing values combined](./data_zero_missing_frequency.png) <br> The number of available accounts has been dropped to **110** from **267**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **data_zero_frequency.csv , data_missingvalue_frequency.csv** are showing the stats for the above analysis.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Still there are some records having missing values and zero value , and those records needs to be dropped** , again the threshold is 30 percent. The Histogram below is showing records with mentioned properties. The x-axis is the percentage of invalid values (Zero and Missing). <br>![Histogram of Records with Invalid values](./data_histogram_records_invalidvalues.png)\n",
    "<br> droping training records with high missing values or zeros at rows , records drop to **57022 from 63360 == 6338 ; 10%** \n",
    "<br> droping testing records with high missing values or zeros at rows , records drop to **4588 from 5280 == 692 ; 13 %**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3- Imputation of accounts <br>\n",
    "> Imputation of just **missing values** is done by mean strategy , for the further step I will try KNN methods or other iterative imputers as they need to be run on HPC. However , the **zero values** was not imputed and I think they should be if we treat them as missing values. The imputation of trainset and test set was done seperatly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4- KPI construction :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5- Analysis of invalid values (zero , nan , inf ) at KPI level :<br>\n",
    ">The figure below illustrates the accumulative percentage of all invalid values , \n",
    "<br><font color = red> Red bars are sowing nan values</font> : Nan values are due to division of zero by zero \n",
    "<br><font color = blue> Blue bars are sowing infinity values</font> : Inf values are due to division of a number by zero \n",
    "<br><font color = green> Green bars are sowing nan values</font> : Zero values are due to division of zero by a number \n",
    "<br>![KPI Invalid Values](./kpi_all_invalidvalues.png)\n",
    "<br> After droping the features (KPIs) with more than 30 percent of invalid values , it ends up to have 87 features(including the id). The figure below illustrates the frequency of each of invalid values for the survived features.\n",
    "<br>![KPI Invalid Values afyter dropping](./kpi_all_invalidvalues_afterdrop.png)\n",
    "<br> Similar to previous step, As the figure below shows, the examination was done for each records as well and the records with more than 30 percent of invalid values were removed from the datasets. \n",
    "<br>![KPI Invalid Values per record Histogram](./kpi_histogram_records_invalidvalues.png)\n",
    "<br>kpi train record dropped to **55995 from 57022 = 1.8 %**\n",
    "<br>kpi test records dropped to **4516 from 4588 = 1.5 %**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- EDA (Exploratory Data Analysis) :\n",
    ">  Let's have a dirty visualization of the features of our interest. \n",
    "<br> According to the proposal , the relation between Gross Profit Per New vehicle and Sales Effectiveness is the point of interest. There are multiple GPNVs and SEs with lacking of the information; among trhem **GPNU(total car and truck retail)**  and the **SE_CJDR** are selected. Later, The rest would be dropped in order to avoid multi_colinearity.  \n",
    "\n",
    "> let's have a dirty analysis on the features first : The polot below is showing how GPNV-SE coordinates scattered. Noteworthy to mention that it is data before removing/correcting outliers. \n",
    "<br> ![GPNV-SE scatter plot before removing outliers](./GPNV_SE.png)\n",
    "<br> As it could be seen it does not show any specific pattern. What would the plot look like after dropping the outliers? \n",
    "<br>\n",
    "<br> ![GPNV-SE scatter plot AFTER removing outliers](./GPNV_SE_WO_outliers.png) \n",
    "<br> for now outliers are just removed but later on , we will correct them and clip them back to the maximum , In this analysis just **5th percentile** of each end were removed. However , a couple of feature might not be nothing but for a dataset with many features we may loos lots of records. \n",
    "\n",
    "> **To have the better understanding , we may look at the data scattered but with different labels.** Meaning that, we may not be able to find a universal pattern , but we could explain the relation if we categorized our dealers based on size, profitability and regions,etc. <br> The figures below illustrates the scattering of data based on size, profitability and regions for which account EXP1 (Expected sale) , O27 (Total CDJR Car and Truck Retail - Sales) and regions were added as lables. For expected sale , each dealer were fallen into one of small, med or big size and regarding the Total sale , each dealer were labeled as low, med or high profitable dealer. Region is already a categorical feature. \n",
    "> The figures below shows how scattered the data is based on above-mentioned features. \n",
    ">### pattern of data based on their dealer size \n",
    "<br> ![GPNV-SE scatter Dealer Size](./GPNV_SE_WO_outliers_dealersize.png)\n",
    "<br> ![dealer size pie chart ](./piechart_dealersize.png)\n",
    "> ### pattern of data based on their total sale \n",
    "<br> ![GPNV-SE scatter Total Sale](./GPNV_SE_WO_outliers_totalsale.png) \n",
    "<br> ![dealer Sale Scale pie chart ](./piechart_dealerSaleScale.png)\n",
    "> ### pattern of data based on their regions \n",
    "<br> ![GPNV-SE scatter REegions](./GPNV_SE_WO_outliers_regional.png)\n",
    "<br> ![dealer Regions pie chart ](./piechart_dealer_regions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Training and Learning ; Dirty Models:\n",
    "> The models below are Random forest and Lasso regression whic at the first glance , looks like promissing because of good R_squared but infacet they are highly suffering from a multi-colinearity. \n",
    ">\n",
    "\n",
    "<br> ![GPNV-Lasso Regression](./LassoRegression_default_GP_vs_prediction.png) \n",
    "<br> ![GPNV-Lasso Regression feature importance ](./lasso_default_featureanalysis.png) \n",
    "<br> ![GPNV-Random Forest](./RandomForest_default_GP_vs_prediction.png) \n",
    "<br> ![GPNV-Random Forest feature importance ](./RandomForest_default_featureanalysis.png)\n",
    "> As we are interseted in explianing GPNV based on SE and other KPIs, we need to remove highly correlate, the heatmap illustrates the correlation between features. \n",
    "<br> ![Correlation between KPIs ](./correlation_heatmap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation is transitive ??? NO , we could not just haphazardly drop one of two high correlated features <br> I need to come up with a way to collect higly correlated features and use the best one to let him play in regression model \n",
    "<br> For time being , I just drop some features about GPNV and SE manually and fot another random Forest MOdel . \n",
    "<br> ![Correlation between KPIs ](./RandomForest_default_GP_vs_prediction_removed_corelation_mannually.png)\n",
    "<br> ![Correlation between KPIs ](./RandomForest_default_Removed_Correlation_mannualy_featureanalysis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 3- Future works ; \n",
    "    > MOre EDA \n",
    "    > Implement model based clustering \n",
    "    > Implement a smart way o get rid of correlation among the features (not manullay remove)\n",
    "    > hyper tuning on Random Forest and Lasso regression \n",
    "    > Neural Net \n",
    "    > Explainable AI \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- In Market Timing : \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
